---
title: "BAN400 Exam Fall 2021"
output: html_document
author: "Candidate 21"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Note: In order for the Rmd file to work, the file needs to be placed in the same working directory where the "data"-folder is located.

We need to load the following packages to execute our code

```{r, include=TRUE, comment="", message=FALSE, warning=FALSE}
library(httr)
library(jsonlite)
library(ggplot2)
library(DescTools)
library(tidyverse)
library(magrittr)
library(rlang)
library(lubridate)
library(anytime)
library(purrr)
library(mdsr) 
library(sf)  
library(ggmap)  
library(ggplot2)
library(rnaturalearth)
library(rnaturalearthdata)
library(rgeos)
library(sf)
library(ggspatial)
library(dplyr)
library(tidymodels)
library(knitr)
library(bannerCommenter)
library(modelr)
library(parallel)
library(furrr)
library(multidplyr)
```

# Problem 1)

A)

We load our json file with the read_json function and format the frame in the same manner as the csv-file.
The data frames are imported and modified in the same manner as the csv file. Some variables had to be converted to a certain date and numeric format.

```{r, include=FALSE, comment="", message=FALSE}
df_202101_csv <-
  read_csv("data/01.csv")

df_202101_json <-
  read_json("data/01.json",
            simplifyVector = TRUE) %>%
  as_tibble() %>%
  mutate(started_at = as.POSIXct(started_at, tz = "Europe/Berlin")) %>% #Change from character to date format. In the CSV file the dates have the POSIXct format.
  mutate(ended_at = as.POSIXct(ended_at, tz = "Europe/Berlin")) %>% #Change from character to date format
  mutate(
    start_station_id = as.numeric(start_station_id),
    end_station_id = as.numeric(end_station_id)
  )


```

B)
and then we make a function to test whether our to dataframes are equal. 
The function is based on the anti_join function which returns all rows from a data frame x that  has no match in y. 
If the number of columns and rows in the tmpdf is equal to the ones in the json data frame, it means that all the columns and rows are identical.


```{r, include=TRUE, comment="", message=FALSE, warning=FALSE}
test_equal_df <- function(df1, df2) {
  tmpdf <-  anti_join(df1, df2)
  if (ncol(tmpdf) - ncol(df2) == 0 & 
      nrow(tmpdf) - nrow(df2) == 0) {
    print("The columns and the values in the two dataframes are the same")
  } else{
    print("The columns and the values are not the same in the two dataframes")
  }
}

test_equal_df(df_202101_csv, df_202101_json)

```


```{r, include=FALSE, comment="", message=FALSE, warning=FALSE}
rm(df_202101_csv, df_202101_json, test_equal_df)
```


# Problem 2) 

First we load our data in a dataframe by mapping through the json files in our folder. Then we tidy the data, and modify our columns to match the requirements. To get the spe icific hour in which the ride was started, we use the floor() function.

The most time consuming process in this file is the loading of the json files. We therefore use the purr package to take advantage of multiple cores of the computer in this process. The effect can be seen in the task manager on windows, or activity monitor on a Mac. The processing time seems to be some seconds faster than without this code.

```{r, include=TRUE, comment="", message=FALSE, warning=FALSE}

Cores <- detectCores() - 1

plan(multisession,
     workers = Cores)

require(dplyr)
require(jsonlite)
require(tidyverse)


citybike_raw <-
  list.files(path = "./data/",
             pattern = "*json",
             full.names = TRUE) %>%
  future_map_dfr( ~ read_json(.), simplifyVector = TRUE) %>%
  as_tibble()

```
Working with the raw data also is a time consuming processs. Multidplyr can be used to work with our data frames
to speed up the process by assigning processing power to more cores. Note that overhead processes might slow the time here as well due to nodes working together. We assign one group to each node in this case.

Using the multidplyr solution cut the total rmd compiling time with about 15 seconds on the first try.
```{r, include=TRUE, comment="", message=FALSE, warning=FALSE}

group <- rep(1:Cores,
             length.out = nrow(citybike_raw))
citybike_raw <- bind_cols(tibble(group),
                          citybike_raw)
citybike_raw

cluster <- new_cluster(Cores)
cluster_library(cluster,
                "dplyr")

citybike_raw <- citybike_raw %>%
  group_by(group) %>%
  partition(cluster) %>%
  mutate(
    floor_start_dh = as.POSIXct(started_at, tz = "Europe/Berlin"),
    ended_at = as.POSIXct(ended_at, tz = "Europe/Berlin"),
    #Change from character to date format
    start_station_id = as.numeric(start_station_id),
    end_station_id = as.numeric(end_station_id),
    start_date = as.Date(floor_start_dh),
    start_hour = lubridate::hour(floor_start_dh),
    weekday_start = lubridate::wday(floor_start_dh),
    floor_start_dh = lubridate::floor_date(floor_start_dh,
                                           unit = "hour")
  ) %>%
  collect() %>%
  ungroup() %>%
  select(-group) %>%
  distinct()

```



Then we check how many observations we have per station

```{r, include=FALSE, comment="", message=FALSE, warning=FALSE}
citybike_raw %>%
  ggplot(aes(start_station_id)) +
  geom_histogram(bins=1000) +
  xlab("Start Station Id") +
  ylab("Count of Start Station ID") +
  theme_classic()
```

Based on the plot above we filter on the stations with at least 15000 observations due to memory constraints and R repeadetly crashing when working with the full data set.

The key to solving this problem lies in populating the missing values for the dates and hours where the count of rides are 0. We use the complete() function to do so. At the same time we fill all the n_rides that have no values with NA.

We complete our data frame by populating the NA's of count of rides with 0 and generally fulfill the requirements of the task. Note that the code also filters on whether the stations are operational. I assume that the cumsum of the n_rides has to be greater than 0 at the start of the set in order to be operational.
If we arrange the data frame in descending order when we group by start_station_id and arrange in descending order by the floor_start_dh, we can filter again on the cumsum of nrides > 0. As long as the rides are = 0 in either end.
We assume the stations are not operational when the count of rides are 0 at the tails of the data set.

```{r, include=TRUE, comment="", message=FALSE, warning=FALSE}
create_df_agg <- function() {
  df_agg <- citybike_raw %>%
    group_by(start_station_id) %>%
    filter(n() > 15000) %>%
    ungroup() %>%
    select(start_station_id,
           floor_start_dh,
           start_hour,
           weekday_start) %>%
    arrange(start_station_id,
            floor_start_dh) %>%
    group_by(floor_start_dh,
             start_station_id) %>%
    mutate(n_rides = n()) %>%
    summarise(n_rides) %>%
    group_by(start_station_id)
  
  #We filter our set based on the count of observations linked to each station. Setting the filter to count > 15000
  # because of processing issues in the further problems. Crucial to this problem is also to group our set by the start stations.
  
  #The following lines will create a frame for each possible
  #combination of the hours and dates for the floor_start_dh column for our given set.
  pos.all <-
    seq(min(df_agg$floor_start_dh),
        max(df_agg$floor_start_dh),
        by = "1 hour")
  
  #The following is the key to solving the problem. We complete our data frame by filling in the missing positions for the
  #floor_start_dh such that we have one observation for each hour and date of the set inside the given time period.
  #At the same time we fill the missing n_rides values with NA for now. This will obviously be 0 in the result.
  df_comp <-
    complete(df_agg,
             floor_start_dh = pos.all,
             fill = list(n_rides = NA))
  
  
  df_agg <- df_comp %>%
    mutate(
      n_rides = (replace_na(n_rides, 0)),
      start_hour = lubridate::hour(floor_start_dh),
      weekday_start = (wday(floor_start_dh))
    ) %>%
    distinct() %>%
    filter(cumsum(n_rides) > 0) %>%
    arrange(desc(floor_start_dh)) %>%
    filter(cumsum(n_rides) > 0) %>%
    arrange(floor_start_dh, start_station_id) %>%
    ungroup() %>%
    mutate(start_hour = factor(start_hour),
           weekday_start = factor(weekday_start))
  
  return(df_agg)
}

df_agg <- create_df_agg()


```


Finally we check if our data frame matches the criterias given.

```{r, include=TRUE, comment="", message=FALSE, warning=FALSE}
library(assertthat)
assert_that(
  df_agg %>%
    group_by(start_station_id,
             floor_start_dh) %>%
    summarise(n = n()) %>%
    ungroup() %>%
    summarise(max_n = max(n)) %$%
    max_n == 1,
  msg = "Duplicates on stations/hours/dates"
)
```

```{r, include=TRUE, comment="", message=FALSE, warning=FALSE}
assert_that(
  df_agg %>%
    group_by(start_station_id) %>%
    mutate(timediff = floor_start_dh - lag(floor_start_dh,
                                           order_by = floor_start_dh)) %>%
    filter(as.numeric(timediff) != 1) %>%
    nrow(.) == 0,
  msg = "Time diffs. between obs are not always 1 hour"
)


```

The code is not optimal due to the heavy processing power needed to do this task initially.
Therefore the data frames are updated gradually so it is easier to work with. Optimally the frames would be made in fewer separate data frames using pipes.


# Problem 3)

First we nest our data frame so we only have one observation per station.
```{r, include=TRUE, comment="", message=FALSE, warning=FALSE}
by_station <- df_agg %>%
  group_by(start_station_id) %>%
  nest()

```

Then we define our regression model as a function so we can apply the model to each station.
```{r, include=TRUE, comment="", message=FALSE, warning=FALSE}
reg_model <- function(df) {
  lm(n_rides ~ weekday_start + start_hour, data = df)
}


```

Now we can map through the lists using the map and map2 functions to estimate the models to each stations, and then add the predictions using the add_predictions with the map2 function. We need to use map2 here because we traverse through both the data and the model lists to get to the predictions

The by_station data frame contains lists of the models with coefficients and residuals for each station, in addition to our predictions. 
The regression model minimizes the squared residuals and fits the regression line in this manner based on our observations. From 
this we receive our predicted values.

```{r, include=TRUE, comment="", message=FALSE, warning=FALSE}
by_station <- by_station %>%
  mutate(model = map(data,
                     reg_model),
         pred = map2(data,
                     model,
                     add_predictions))


```



```{r, include=FALSE, comment="", message=FALSE, warning=FALSE}
create_stations <- function() {
  stations <- citybike_raw %>%
    select(start_station_id,
           start_station_name) %>%
    distinct() #These small dataframes are helpful because R has repeatedly crashed during this assignment.
  return(stations)
}


pred <- by_station %>%
  unnest(pred) %>%
  select(start_station_id,
         start_hour,
         weekday_start,
         pred) %>%
  distinct() %>%
  arrange(start_station_id,
          start_hour) %>%
  left_join(create_stations(),
            by = "start_station_id")

days <-
  c("Monday",
    "Tuesday",
    "Wednesday",
    "Thursday",
    "Friday",
    "Saturday",
    "Sunday")
```

After tidying up our data frame we can create a plot of the predicted rides for each station throughout the week. The pred data frame is the solution to this task.

```{r, include=TRUE, comment="", message=FALSE, warning=FALSE}
pred %>%
  group_by(start_hour,
           weekday_start,
           start_station_id) %>%
  distinct()  %>%
  mutate(weekday = factor(days[weekday_start],
                          levels = days)) %>%
  ggplot(aes(
    start_hour,
    pred,
    group = as.factor(start_station_id),
    color = as.factor(start_station_name)
  )) +
  geom_point() +
  geom_line() +
  facet_wrap( ~ weekday, scales = 'free') +
  ylab("Mean Predicted Rides Per Hour Per Station") +
  xlab("Hour of day") +
  theme_classic() +
  scale_colour_brewer(palette = "Spectral") +
  labs(title = "Rides throughout day and week for each station\n",
       color = "Start Station Name") +
  theme(plot.title = element_text(size = 18,
                                  color = "black"))
```


# Problem 4)

To save some space we make some functions that gives us the data frames we want in the following.

```{r, include=FALSE, comment="", message=FALSE, warning=FALSE}
create_station_coordinates <- function() {
  station_coordinates <- citybike_raw %>%
    select(
      start_station_id,
      start_station_latitude,
      start_station_longitude,
      start_date,
      start_station_name
    ) %>%
    distinct() %>%
    mutate(lat = start_station_latitude,
           lon = start_station_longitude)
  return(station_coordinates)
}

```


In order to account for changing of location of stations, we have to include the date column in the coordinates for the stations. 
We merge the coordinates with a left_join and tidy our data in order to make our "map" function.

```{r, include=TRUE, comment="", message=FALSE, warning=FALSE}
create_df_agg_coord <- function() {
  df_agg_coord <- df_agg %>%
    mutate(start_date = as.Date(floor_start_dh)) %>%
    left_join(create_station_coordinates(),
              by = c("start_station_id", "start_date")) %>%
    select(start_station_id,
           n_rides,
           start_hour,
           start_date,
           lat,
           lon,
           start_station_name) %>%
    mutate(start_date = format(as.Date(start_date),
                               '%d-%m-%Y'))
  return(df_agg_coord)
}

```

We create our function with the inputs of date and hour of the day. Now we can enter given dates and hours and see how active our stations are at the given hours and dates. The function consists of a temporary dataframe that filters our given input so we can summarise based on this. After the dataframe is created we create a map using the ggmap, make_bbox and get_map functions.

Make sure to use the right date format when using the function. "" must be wrapped around the date input for the function to work.

```{r, include=TRUE, comment="", message=FALSE, warning=FALSE}
map_datehour <- function(date, hour) {
  tmpdf <- create_df_agg_coord() %>%
    select(start_date,
           start_hour,
           n_rides,
           start_station_id,
           start_station_name,
           lat,
           lon) %>%
    filter(start_date == date,
           start_hour == hour)
  bergen <-
    make_bbox(tmpdf$lon, #Være nøye med lat lon rekkefølgen!
              tmpdf$lat,
              f = .05) #Expand by 5 %
  m <- get_map(bergen, zoom = 15, source = "osm")
  ggmap(m) +
    geom_point(aes(
      x = lon,
      y = lat,
      size = n_rides,
      color = as.factor(start_station_name)
    ), data = tmpdf) +
    scale_colour_brewer(palette = "Spectral") +
    labs(title = "Activity of the stations for given dates and hours\n", color = "Start Station Name") +
    theme(plot.title = element_text(size = 18, color = "black")) +
    ylab("Latitude") +
    xlab("Longitude") +
    labs(size = "Number \nOf Rides")
  
}

map_datehour("01-10-2021", 15)
```
